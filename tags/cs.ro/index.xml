<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cs.RO on Evofutura</title><link>https://evofutura.com/tags/cs.ro/</link><description>Recent content in Cs.RO on Evofutura</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 30 Jan 2026 05:00:00 +0000</lastBuildDate><atom:link href="https://evofutura.com/tags/cs.ro/index.xml" rel="self" type="application/rss+xml"/><item><title>Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies</title><link>https://evofutura.com/posts/2026-01-30-abstracting-robot-manipulation-skills-via-mixture-of-experts/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-abstracting-robot-manipulation-skills-via-mixture-of-experts/</guid><description>&lt;p&gt;arXiv:2601.21251v1 Announce Type: new
Abstract: Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change.&lt;/p&gt;</description></item><item><title>Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter</title><link>https://evofutura.com/posts/2026-01-30-deep-qp-safety-filter-model-free-learning-for-reachability/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-deep-qp-safety-filter-model-free-learning-for-reachability/</guid><description>&lt;p&gt;arXiv:2601.21297v1 Announce Type: new
Abstract: We introduce Deep QP Safety Filter, a fully data-driven safety layer for black-box dynamical systems. Our method learns a Quadratic-Program (QP) safety filter without model knowledge by combining Hamilton-Jacobi (HJ) reachability with model-free learning. We construct contraction-based losses for both the safety value and its derivatives, and train two neural networks accordingly. In the exact setting, the learned critic converges to the viscosity solution (and its derivative), even for non-smooth values. Across diverse dynamical systems &amp;ndash; even including a hybrid system &amp;ndash; and multiple RL tasks, Deep QP Safety Filter substantially reduces pre-convergence failures while accelerating learning toward higher returns than strong baselines, offering a principled and practical route to safe, model-free control.&lt;/p&gt;</description></item><item><title>Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation</title><link>https://evofutura.com/posts/2026-01-30-disturbance-aware-flight-control-of-robotic-gliding-blimp-vi/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-disturbance-aware-flight-control-of-robotic-gliding-blimp-vi/</guid><description>&lt;p&gt;arXiv:2601.21188v1 Announce Type: new
Abstract: Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.&lt;/p&gt;</description></item><item><title>HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control</title><link>https://evofutura.com/posts/2026-01-30-hptune-hierarchical-proactive-tuning-for-collision-free-mod/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-hptune-hierarchical-proactive-tuning-for-collision-free-mod/</guid><description>&lt;p&gt;arXiv:2601.21346v1 Announce Type: new
Abstract: Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.&lt;/p&gt;</description></item><item><title>InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios</title><link>https://evofutura.com/posts/2026-01-30-inspecsafe-v1-a-multimodal-benchmark-for-safety-assessment/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-inspecsafe-v1-a-multimodal-benchmark-for-safety-assessment/</guid><description>&lt;p&gt;arXiv:2601.21173v1 Announce Type: new
Abstract: With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.&lt;/p&gt;</description></item><item><title>Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems</title><link>https://evofutura.com/posts/2026-01-30-meta-ros-a-next-generation-middleware-architecture-for-adap/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-meta-ros-a-next-generation-middleware-architecture-for-adap/</guid><description>&lt;p&gt;arXiv:2601.21011v1 Announce Type: new
Abstract: The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS&amp;rsquo;s performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.&lt;/p&gt;</description></item><item><title>Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned</title><link>https://evofutura.com/posts/2026-01-30-multi-robot-decentralized-collaborative-slam-in-planetary-an/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-multi-robot-decentralized-collaborative-slam-in-planetary-an/</guid><description>&lt;p&gt;arXiv:2601.21063v1 Announce Type: new
Abstract: Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure. This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets. In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network. We examine the impact of limited and intermittent communication on C-SLAM performance, as well as the unique localization challenges posed by planetary-like environments. Additionally, we introduce a novel dataset collected during our experiments, which includes real-time peer-to-peer inter-robot throughput and latency measurements. This dataset aims to support future research on communication-constrained, decentralized multirobot operations.&lt;/p&gt;</description></item><item><title>Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs</title><link>https://evofutura.com/posts/2026-01-30-quick-heuristic-validation-of-edges-in-dynamic-roadmap-graph/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-quick-heuristic-validation-of-edges-in-dynamic-roadmap-graph/</guid><description>&lt;p&gt;arXiv:2601.20968v1 Announce Type: new
Abstract: In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the &amp;ldquo;Red-Green-Gray&amp;rdquo; paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.&lt;/p&gt;</description></item><item><title>Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing</title><link>https://evofutura.com/posts/2026-01-30-track-centric-iterative-learning-for-global-trajectory-optim/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-track-centric-iterative-learning-for-global-trajectory-optim/</guid><description>&lt;p&gt;arXiv:2601.21027v1 Announce Type: new
Abstract: This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.&lt;/p&gt;</description></item><item><title>WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics</title><link>https://evofutura.com/posts/2026-01-30-wheelarm-sim-a-manipulation-and-navigation-combined-multimo/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-wheelarm-sim-a-manipulation-and-navigation-combined-multimo/</guid><description>&lt;p&gt;arXiv:2601.21129v1 Announce Type: new
Abstract: Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.&lt;/p&gt;</description></item></channel></rss>