<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Eess.SY on Evofutura</title><link>https://evofutura.com/tags/eess.sy/</link><description>Recent content in Eess.SY on Evofutura</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 30 Jan 2026 05:00:00 +0000</lastBuildDate><atom:link href="https://evofutura.com/tags/eess.sy/index.xml" rel="self" type="application/rss+xml"/><item><title>Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter</title><link>https://evofutura.com/posts/2026-01-30-deep-qp-safety-filter-model-free-learning-for-reachability/</link><pubDate>Fri, 30 Jan 2026 05:00:00 +0000</pubDate><guid>https://evofutura.com/posts/2026-01-30-deep-qp-safety-filter-model-free-learning-for-reachability/</guid><description>&lt;p&gt;arXiv:2601.21297v1 Announce Type: new
Abstract: We introduce Deep QP Safety Filter, a fully data-driven safety layer for black-box dynamical systems. Our method learns a Quadratic-Program (QP) safety filter without model knowledge by combining Hamilton-Jacobi (HJ) reachability with model-free learning. We construct contraction-based losses for both the safety value and its derivatives, and train two neural networks accordingly. In the exact setting, the learned critic converges to the viscosity solution (and its derivative), even for non-smooth values. Across diverse dynamical systems &amp;ndash; even including a hybrid system &amp;ndash; and multiple RL tasks, Deep QP Safety Filter substantially reduces pre-convergence failures while accelerating learning toward higher returns than strong baselines, offering a principled and practical route to safe, model-free control.&lt;/p&gt;</description></item></channel></rss>